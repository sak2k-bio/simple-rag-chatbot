# ðŸš€ Bulk PDF Embeddings - Production Ready

This folder contains production-ready scripts for bulk processing PDFs with optimized chunking and paragraph awareness.

## ðŸ“ Contents

- **`bulk-pdf-processor-optimized.js`** - Main bulk PDF processing script with advanced features
- **`web-ui/`** - Web interface for easy management and monitoring
- **`Dockerfile`** - Docker configuration for containerized deployment
- **`docker-compose.yml`** - Complete stack with Qdrant database
- **`package.json`** - Dependencies and scripts
- **`env.example`** - Environment variables template
- **`setup.sh`** - Automated setup script for Linux/Mac
- **`setup.bat`** - Automated setup script for Windows
- **`start-ollama-gpu.sh`** - GPU-optimized Ollama startup (Linux/Mac)
- **`start-ollama-gpu.bat`** - GPU-optimized Ollama startup (Windows)
- **`README.md`** - This comprehensive guide

## ðŸŽ¯ What This Does

The bulk PDF processor is designed for **production-scale PDF ingestion** with:

### âœ¨ **Advanced Features:**
- ðŸ§  **Optimal Chunking** - 800-1200 character chunks with semantic boundaries
- ðŸ“„ **Paragraph Awareness** - Respects document structure (paragraphs, sentences, words)
- ðŸ”„ **Resume Capability** - Can resume interrupted processing sessions
- ðŸ“Š **Progress Tracking** - Real-time progress updates and statistics
- ðŸ›¡ï¸ **Error Handling** - Robust error handling with retry logic
- ðŸ’¾ **Manifest Database** - SQLite database tracks processed files
- âš¡ **Concurrent Processing** - Parallel processing with configurable concurrency
- ðŸ·ï¸ **Rich Metadata** - Enhanced metadata extraction for medical content
- ðŸ¦™ **Dual Embedding Support** - Google AI (cloud) or Ollama (local, privacy-focused)

### ðŸŽ¯ **Optimized for Medical Content:**
- âœ… **Medical Term Detection** - Automatically extracts relevant medical terms
- âœ… **Page Number Extraction** - Preserves page references
- âœ… **Chapter/Section Parsing** - Extracts document structure
- âœ… **Flexible Embeddings** - Google AI or local Ollama models

## ðŸš€ Quick Start

### Option 1: Docker (Recommended)

**Easiest way to get started with a complete setup:**

#### ðŸš€ **Quick Setup (Automated):**
```bash
# 1. Navigate to the bulk_embeddings directory
cd bulk_embeddings

# 2. Run the setup script
# On Linux/Mac:
chmod +x setup.sh && ./setup.sh

# On Windows:
setup.bat
```

#### ðŸ”§ **Manual Setup:**
```bash
# 1. Clone and navigate to the bulk_embeddings directory
cd bulk_embeddings

# 2. Copy environment template
cp env.example .env.local

# 3. Edit .env.local with your API keys
# Add your GOOGLE_API_KEY and other settings

# 4. Start the complete stack (includes Qdrant)
docker-compose up -d

# 5. Open web UI at http://localhost:3001
# Upload PDFs and start processing!
```

### Option 2: Manual Setup

**For development or custom configurations:**

1. **Environment Setup:**
   ```bash
   # Copy environment template
   cp env.example .env.local
   
   # Edit .env.local with your settings:
   GOOGLE_API_KEY=your_google_api_key
   QDRANT_URL=http://localhost:6333
   QDRANT_API_KEY=your_qdrant_api_key
   QDRANT_COLLECTION=pulmo_fishman
   ```

2. **Dependencies:**
   ```bash
   npm install
   ```

3. **Start Qdrant (if not using Docker):**
   ```bash
   docker run -p 6333:6333 qdrant/qdrant:latest
   ```

4. **Start Web UI:**
   ```bash
   npm start
   # Open http://localhost:3001
   ```

### Option 3: Command Line Only

```bash
# Process all PDFs in a directory
node bulk-pdf-processor-optimized.js "C:\path\to\your\pdfs"
```

## ðŸ–¥ï¸ Web UI Features

The included web interface provides a user-friendly way to manage bulk PDF processing:

### âœ¨ **Key Features:**
- ðŸ“ **Drag & Drop Upload** - Easy PDF file uploads
- âš™ï¸ **Visual Configuration** - Adjust processing settings with sliders and inputs
- ðŸ“Š **Real-time Monitoring** - Live progress tracking and statistics
- ðŸ“‹ **Processing Logs** - Detailed logs with timestamps
- ðŸŽ¯ **Collection Stats** - View Qdrant collection information
- ðŸ›‘ **Process Control** - Start, stop, and monitor processing jobs
- ðŸ“± **Responsive Design** - Works on desktop and mobile devices

### ðŸŽ¯ **Web UI Workflow:**
1. **Upload PDFs** - Drag & drop or browse for PDF files
2. **Configure Settings** - Adjust chunk size, concurrency, batch sizes
3. **Start Processing** - Click "Start Processing" to begin
4. **Monitor Progress** - Watch real-time progress and logs
5. **View Results** - Check collection statistics and processing results

### ðŸŒ **Access the Web UI:**
- **Docker:** http://localhost:3001
- **Manual:** http://localhost:3001 (after running `npm start`)

## ðŸ¦™ Embedding Providers

The system supports two embedding providers for maximum flexibility:

### ðŸ” **Google AI (Cloud-based)**
- **Model:** `text-embedding-004`
- **Dimensions:** 768
- **Pros:** High quality, fast, reliable
- **Cons:** Requires API key, data sent to Google
- **Best for:** Production environments, high-volume processing

### ðŸ¦™ **Ollama (Local, Privacy-focused)**
- **Model:** `nomic-embed-text` (recommended)
- **Dimensions:** 768
- **Pros:** Completely local, no API costs, privacy-focused, GPU acceleration
- **Cons:** Requires local setup, GPU setup complexity
- **Best for:** Privacy-sensitive environments, offline processing, GPU-accelerated processing

### ðŸš€ **Setting up Ollama with GPU Acceleration:**

1. **Install Ollama:**
   ```bash
   # Visit https://ollama.ai for installation instructions
   curl -fsSL https://ollama.ai/install.sh | sh
   ```

2. **Install the embedding model:**
   ```bash
   ollama pull nomic-embed-text
   ```

3. **Start Ollama with GPU acceleration:**
   ```bash
   # Option 1: Use the provided startup script (recommended)
   ./start-ollama-gpu.sh    # Linux/Mac
   start-ollama-gpu.bat     # Windows
   
   # Option 2: Manual startup
   OLLAMA_GPU_LAYERS=20 ollama serve
   ```

4. **Configure environment:**
   ```bash
   # In your .env.local file:
   EMBEDDING_PROVIDER=ollama
   OLLAMA_URL=http://localhost:11434
   OLLAMA_MODEL=nomic-embed-text
   
   # GPU-optimized settings
   BULK_CONCURRENCY=8
   BULK_EMBED_BATCH=32
   OLLAMA_GPU_LAYERS=20
   OLLAMA_NUM_CTX=2048
   OLLAMA_NUM_BATCH=512
   ```
-----
options: {
  num_gpu: 1,           // Use 1 GPU
  num_ctx: 2048,        // Context window
  num_batch: 512,       // GPU batch size
  num_thread: 4,        // CPU preprocessing threads
  use_mmap: true,       // Memory mapping
  use_mlock: true       // Lock memory
}

----


### ðŸš€ **GPU Performance Optimization:**

#### **For 12GB VRAM (Your Setup):**
- **Concurrency:** 8 (increased from 6)
- **Embed Batch:** 32 (optimized for GPU memory)
- **GPU Layers:** 20 (utilizes most of your VRAM)
- **Context Window:** 2048 (optimal for embeddings)
- **Batch Size:** 512 (GPU processing batch)

#### **Expected Performance:**
- **Speed:** 3-5x faster than CPU-only Ollama
- **Throughput:** ~50-100 texts/second (vs ~10-30 CPU-only)
- **Memory Usage:** ~8-10GB VRAM utilization
- **Processing Time:** 100 PDFs in ~20-40 minutes (vs 60-90 minutes CPU-only)

### ðŸŽ¯ **Web UI Configuration:**
The web interface allows you to switch between providers and configure Ollama settings:
- Select embedding provider (Google AI or Ollama)
- Configure Ollama URL and model
- Test Ollama connection status
- View available models

### Example Commands

```bash
# Process with Google AI embeddings (default)
node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\Users\User\Desktop\medical_pdfs"

# Process with Ollama embeddings (local, privacy-focused)
EMBEDDING_PROVIDER=ollama node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\Users\User\Desktop\medical_pdfs"

# Process with custom Ollama model
EMBEDDING_PROVIDER=ollama OLLAMA_MODEL=mxbai-embed-large node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\data\pdfs"

# Process with custom settings (via environment variables)
BULK_CONCURRENCY=8 OPTIMAL_CHUNK_SIZE=1200 node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\data\pdfs"
```

## âš™ï¸ Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `EMBEDDING_PROVIDER` | `google` | Embedding provider: `google` or `ollama` |
| `GOOGLE_API_KEY` | *required* | Google AI API key (when using Google) |
| `OLLAMA_URL` | `http://localhost:11434` | Ollama server URL (when using Ollama) |
| `OLLAMA_MODEL` | `nomic-embed-text` | Ollama embedding model |
| `BULK_CONCURRENCY` | `8` | Number of files to process simultaneously (GPU-optimized) |
| `BULK_EMBED_BATCH` | `32` | Number of texts to embed in one batch (GPU-optimized) |
| `BULK_UPSERT_BATCH` | `100` | Number of vectors to upsert in one batch |
| `OPTIMAL_CHUNK_SIZE` | `1000` | Target chunk size in characters |
| `OPTIMAL_CHUNK_OVERLAP` | `200` | Overlap between chunks in characters |
| `OLLAMA_GPU_LAYERS` | `20` | GPU layers for 12GB VRAM optimization |
| `OLLAMA_NUM_CTX` | `2048` | Context window size |
| `OLLAMA_NUM_BATCH` | `512` | GPU batch processing size |
| `QDRANT_URL` | `http://localhost:6333` | Qdrant server URL |
| `QDRANT_COLLECTION` | `pulmo_fishman` | Collection name |

### Performance Tuning

#### For Large Datasets (1000+ PDFs):
```bash
# Higher concurrency, larger batches
BULK_CONCURRENCY=12 BULK_EMBED_BATCH=100 BULK_UPSERT_BATCH=200 node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\large_pdf_collection"
```

#### For High-Quality Chunking:
```bash
# Larger chunks with more overlap
OPTIMAL_CHUNK_SIZE=1200 OPTIMAL_CHUNK_OVERLAP=300 node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\medical_pdfs"
```

#### For Fast Processing:
```bash
# Smaller chunks, higher concurrency
OPTIMAL_CHUNK_SIZE=800 BULK_CONCURRENCY=8 node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\pdfs"
```

## ðŸ“Š Monitoring & Progress

### Real-Time Progress
The script provides detailed progress updates:
```
[2025-01-03T23:13:45.123Z] â„¹ï¸ ðŸ“ˆ Progress: 45.2% (23/51) - medical_textbook_chapter_5.pdf - Elapsed: 180s
[2025-01-03T23:13:46.456Z] â„¹ï¸ âœ… Successfully processed: medical_textbook_chapter_5.pdf (47 optimized chunks)
```

### Manifest Database
The script creates `bulk_manifest_optimized.db` to track:
- âœ… **Completed files** - Successfully processed
- âŒ **Failed files** - With error messages
- â­ï¸ **Skipped files** - Already processed (unchanged)
- ðŸ“Š **Statistics** - Total chunks, processing time, etc.

### Resume Capability
If processing is interrupted, simply run the same command again:
- âœ… **Already processed files** are automatically skipped
- ðŸ”„ **Failed files** are retried
- ðŸ“ˆ **Progress continues** from where it left off

## ðŸŽ¯ Chunking Strategy

### Semantic Boundary Detection
The script uses intelligent chunking that respects document structure:

1. **Paragraph Boundaries** (Priority 1)
   - Breaks at `\n\n` (double newlines)
   - Preserves paragraph structure

2. **Sentence Boundaries** (Priority 2)
   - Breaks at `.` (periods)
   - Maintains sentence integrity

3. **Word Boundaries** (Priority 3)
   - Breaks at spaces
   - Avoids splitting words

### Chunk Size Optimization
- **Target Size:** 800-1200 characters
- **Overlap:** 200 characters between chunks
- **Minimum Size:** 50 characters (filters out tiny chunks)

### Example Chunking:
```
Original Text: "Asthma is a chronic inflammatory disease of the airways. 
It affects millions of people worldwide. The condition is characterized by 
recurrent episodes of wheezing, breathlessness, chest tightness, and coughing."

Chunk 1: "Asthma is a chronic inflammatory disease of the airways. 
It affects millions of people worldwide. The condition is characterized by 
recurrent episodes of wheezing, breathlessness, chest tightness, and coughing."

Chunk 2: "The condition is characterized by recurrent episodes of wheezing, 
breathlessness, chest tightness, and coughing. [Next paragraph content...]"
```

## ðŸ·ï¸ Metadata Extraction

### Automatic Medical Term Detection
The script automatically extracts medical terms from each chunk:

```javascript
const medicalTerms = [
  'asthma', 'copd', 'pneumonia', 'cancer', 'treatment', 'diagnosis', 
  'symptoms', 'therapy', 'medication', 'lung', 'respiratory', 'pulmonary',
  'bronchitis', 'emphysema', 'fibrosis', 'tuberculosis', 'covid', 
  'influenza', 'pneumothorax', 'pleural', 'alveolar', 'bronchial', 
  'tracheal', 'ventilation', 'oxygenation'
];
```

### Document Structure Parsing
Extracts from filename patterns:
- **Page Numbers:** `Page_804_` â†’ `pageNumber: 804`
- **Chapters:** `CHAPTER_74_` â†’ `chapterNumber: 74, chapterTitle: "Pulmonary Vasculitis"`
- **Parts:** `PART_4_` â†’ `partNumber: 4, partTitle: "Obstructive Lung Diseases"`

### Rich Metadata Example
```json
{
  "source": "C:\\pdfs\\Page_804_PART_4_Obstructive_Lung_Diseases.pdf",
  "pageNumber": 804,
  "chapterNumber": 74,
  "chapterTitle": "Pulmonary Vasculitis",
  "partNumber": 4,
  "partTitle": "Obstructive Lung Diseases",
  "keyTerms": ["asthma", "pulmonary", "respiratory", "treatment"],
  "chunkIndex": 2,
  "totalChunks": 15,
  "chunkLength": 1150,
  "optimized": true,
  "timestamp": "2025-01-03T23:13:45.123Z"
}
```

## ðŸ”§ Troubleshooting

### Common Issues

#### 1. **"Google Generative AI API key is missing"**
```bash
# Solution: Add to .env.local
echo "GOOGLE_API_KEY=your_actual_api_key" >> .env.local
```

#### 2. **"No PDF files found in directory"**
```bash
# Check directory path and file extensions
ls "C:\path\to\pdfs"  # Should show .pdf files
```

#### 3. **"Qdrant connection failed"**
```bash
# Check Qdrant is running
curl http://localhost:6333/collections
```

#### 4. **"Embedding failed"**
```bash
# Check API key and rate limits
# Reduce BULK_EMBED_BATCH size
BULK_EMBED_BATCH=25 node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\pdfs"
```

### Performance Issues

#### Slow Processing
```bash
# Increase concurrency
BULK_CONCURRENCY=8 node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\pdfs"
```

#### Memory Issues
```bash
# Reduce batch sizes
BULK_EMBED_BATCH=25 BULK_UPSERT_BATCH=50 node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\pdfs"
```

#### API Rate Limits
```bash
# Reduce concurrency and batch sizes
BULK_CONCURRENCY=3 BULK_EMBED_BATCH=20 node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\pdfs"
```

## ðŸ“ˆ Performance Benchmarks

### Typical Performance (on modern hardware):
- **Small PDFs (1-5 pages):** ~2-5 seconds per file
- **Medium PDFs (10-50 pages):** ~10-30 seconds per file
- **Large PDFs (100+ pages):** ~1-3 minutes per file

### Throughput Examples:
- **100 small PDFs:** ~5-10 minutes
- **50 medium PDFs:** ~15-30 minutes
- **20 large PDFs:** ~30-60 minutes

### Memory Usage:
- **Base memory:** ~100-200 MB
- **Per concurrent file:** ~50-100 MB
- **Peak memory:** ~500 MB - 1 GB (depending on concurrency)

## ðŸ”„ Workflow Examples

### 1. **Initial Bulk Processing**
```bash
# First time processing a large collection
node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\medical_library"
```

### 2. **Adding New PDFs**
```bash
# Add new PDFs to existing collection
node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\new_medical_pdfs"
```

### 3. **Reprocessing with Better Settings**
```bash
# Delete manifest database to force reprocessing
rm bulk_manifest_optimized.db
OPTIMAL_CHUNK_SIZE=1200 node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\pdfs"
```

### 4. **Production Deployment**
```bash
# Production settings for large-scale processing
BULK_CONCURRENCY=8 BULK_EMBED_BATCH=100 BULK_UPSERT_BATCH=200 \
OPTIMAL_CHUNK_SIZE=1000 OPTIMAL_CHUNK_OVERLAP=200 \
node bulk_embeddings/bulk-pdf-processor-optimized.js "C:\production_pdfs"
```

## ðŸŽ¯ Best Practices

### 1. **File Organization**
- Organize PDFs in logical folder structures
- Use descriptive filenames with page numbers
- Keep related documents together

### 2. **Processing Strategy**
- Start with a small test batch
- Monitor progress and adjust settings
- Use resume capability for large datasets

### 3. **Quality Control**
- Check manifest database for errors
- Verify chunk quality with sample queries
- Monitor embedding consistency

### 4. **Performance Optimization**
- Adjust concurrency based on system resources
- Use appropriate batch sizes for your API limits
- Consider chunk size vs. retrieval quality trade-offs

## ðŸ”— Integration with RAG System

After processing, your PDFs are ready for the RAG system:

```javascript
// The processed chunks are automatically available in your Qdrant collection
// with optimized metadata for better retrieval

// Example query in your RAG system:
const results = await client.search(collectionName, {
  vector: queryEmbedding,
  limit: 10,
  filter: {
    must: [
      { key: "metadata.keyTerms", match: { value: "asthma" } },
      { key: "metadata.pageNumber", range: { gte: 800, lte: 900 } }
    ]
  }
});
```

## ðŸ“ž Support

For issues or questions:
1. Check the troubleshooting section above
2. Review the manifest database for error details
3. Check your environment variables and API keys
4. Verify Qdrant connection and collection status

---

**ðŸŽ‰ Happy Processing!** This tool is designed to make bulk PDF processing efficient, reliable, and optimized for medical content retrieval.
